{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a467d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7643ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c0145b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 256)\n",
       "    (token_type_embeddings): Embedding(2, 256)\n",
       "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-3): 4 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the lightweight BERT model and its tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-mini\")\n",
    "model = AutoModel.from_pretrained(\"prajjwal1/bert-mini\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411c2d7",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a35d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: capital-common-countries\n",
      "k     Accuracy (Cosine Similarity)   Accuracy (L2 Distance)        \n",
      "1     64.82                          98.02                         \n",
      "2     76.48                          99.01                         \n",
      "5     86.17                          100.00                        \n",
      "10    94.47                          100.00                        \n",
      "20    99.21                          100.00                        \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Group: capital-world\n",
      "k     Accuracy (Cosine Similarity)   Accuracy (L2 Distance)        \n",
      "1     16.78                          51.99                         \n",
      "2     21.68                          59.86                         \n",
      "5     31.12                          71.24                         \n",
      "10    40.52                          78.69                         \n",
      "20    52.83                          85.08                         \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Group: currency\n",
      "k     Accuracy (Cosine Similarity)   Accuracy (L2 Distance)        \n",
      "1     0.00                           0.00                          \n",
      "2     6.00                           8.89                          \n",
      "5     18.24                          28.52                         \n",
      "10    38.57                          50.81                         \n",
      "20    73.09                          86.72                         \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dictionary to cache computed word embeddings\n",
    "embedding_cache = {}\n",
    "\n",
    "def get_embedding(word):\n",
    "    \"\"\"\n",
    "    Given a word, computes its embedding using the BERT model.\n",
    "    If the word tokenizes into multiple tokens, their embeddings are averaged.\n",
    "    The result is cached for efficiency.\n",
    "    \"\"\"\n",
    "    # Use lower-case since we are using an uncased model.\n",
    "    word = word.lower()\n",
    "    if word in embedding_cache:\n",
    "        return embedding_cache[word]\n",
    "    with torch.no_grad():\n",
    "        # Tokenize without adding special tokens so that only the word’s sub–tokens are processed\n",
    "        inputs = tokenizer(word, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        # outputs.last_hidden_state has shape (1, sequence_length, hidden_size)\n",
    "        token_embeds = outputs.last_hidden_state.squeeze(0)  # shape: (sequence_length, hidden_size)\n",
    "        # If the word is split into multiple tokens, average the token embeddings\n",
    "        if token_embeds.dim() == 1:\n",
    "            embed = token_embeds\n",
    "        else:\n",
    "            embed = token_embeds.mean(dim=0)\n",
    "        embedding_cache[word] = embed\n",
    "        return embed\n",
    "\n",
    "def read_analogy_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads the analogy file and splits it into groups.\n",
    "    Each group starts with a line beginning with a colon (\":\") specifying the group name.\n",
    "    The following lines (until the next group header) are assumed to be analogies\n",
    "    in the format: a b c d\n",
    "    \"\"\"\n",
    "    groups = {}\n",
    "    current_group = None\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # If the line starts with \":\" it indicates a new group.\n",
    "            if line.startswith(\":\"):\n",
    "                group_name = line[1:].strip()\n",
    "                current_group = group_name\n",
    "                groups[current_group] = []\n",
    "            else:\n",
    "                # Each line should contain exactly 4 words\n",
    "                tokens = line.split()\n",
    "                if len(tokens) == 4 and current_group is not None:\n",
    "                    groups[current_group].append(tokens)\n",
    "    return groups\n",
    "\n",
    "def evaluate_group(analogy_list):\n",
    "    \"\"\"\n",
    "    For a given list of analogies (each a list of 4 words [a, b, c, d]),\n",
    "    compute prediction accuracy for varying cutoff values k based on two measures:\n",
    "      - Cosine similarity: higher is better.\n",
    "      - L2 distance: lower is better.\n",
    "    \n",
    "    The candidate set is defined as all unique words that appear in the second (b)\n",
    "    and fourth (d) positions among the analogies in the group.\n",
    "    Returns the list of ks and two dictionaries mapping each k to the accuracy percentage.\n",
    "    \"\"\"\n",
    "    # Build candidate set: unique words from the second and fourth positions in the group\n",
    "    candidates = set()\n",
    "    for tokens in analogy_list:\n",
    "        candidates.add(tokens[1].lower())\n",
    "        candidates.add(tokens[3].lower())\n",
    "    candidates = list(candidates)\n",
    "\n",
    "    ks = [1, 2, 5, 10, 20]\n",
    "    cosine_correct = {k: 0 for k in ks}\n",
    "    l2_correct = {k: 0 for k in ks}\n",
    "    total = len(analogy_list)\n",
    "    \n",
    "    for tokens in analogy_list:\n",
    "        # Unpack the analogy: a is to b as c is to d.\n",
    "        a, b, c, d = [word.lower() for word in tokens]\n",
    "        # Compute the reference difference vector from the known pair (a and b)\n",
    "        ref_diff = get_embedding(a) - get_embedding(b)\n",
    "        \n",
    "        # For each candidate, compute the difference vector with respect to c\n",
    "        cos_scores = {}\n",
    "        l2_scores = {}\n",
    "        for cand in candidates:\n",
    "            candidate_diff = get_embedding(c) - get_embedding(cand)\n",
    "            # For cosine similarity: higher is better\n",
    "            cos_sim = F.cosine_similarity(ref_diff, candidate_diff, dim=0)\n",
    "            # For L2 distance: lower is better\n",
    "            l2_distance = torch.norm(ref_diff - candidate_diff, p=2)\n",
    "            cos_scores[cand] = cos_sim.item()\n",
    "            l2_scores[cand] = l2_distance.item()\n",
    "        \n",
    "        # Sort candidates by cosine similarity (descending order) and by L2 distance (ascending order)\n",
    "        sorted_cos = sorted(cos_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_l2 = sorted(l2_scores.items(), key=lambda x: x[1])\n",
    "        \n",
    "        # For each cutoff k, check if the true answer d is among the top k candidates\n",
    "        for k in ks:\n",
    "            top_k_cos = [item[0] for item in sorted_cos[:k]]\n",
    "            top_k_l2 = [item[0] for item in sorted_l2[:k]]\n",
    "            if d in top_k_cos:\n",
    "                cosine_correct[k] += 1\n",
    "            if d in top_k_l2:\n",
    "                l2_correct[k] += 1\n",
    "                \n",
    "    # Calculate accuracy percentages\n",
    "    cosine_acc = {k: (cosine_correct[k] / total) * 100 for k in ks}\n",
    "    l2_acc = {k: (l2_correct[k] / total) * 100 for k in ks}\n",
    "    \n",
    "    return ks, cosine_acc, l2_acc\n",
    "\n",
    "def print_table(group_name, ks, cosine_acc, l2_acc):\n",
    "    \"\"\"\n",
    "    Prints a formatted results table for a given group.\n",
    "    \"\"\"\n",
    "    print(\"Group:\", group_name)\n",
    "    print(\"{:<5} {:<30} {:<30}\".format(\"k\", \"Accuracy (Cosine Similarity)\", \"Accuracy (L2 Distance)\"))\n",
    "    for k in ks:\n",
    "        print(\"{:<5} {:<30.2f} {:<30.2f}\".format(k, cosine_acc[k], l2_acc[k]))\n",
    "    print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Read the analogy dataset from the local file\n",
    "    file_path = \"./data/task_1_data.txt\"\n",
    "    groups = read_analogy_file(file_path)\n",
    "    \n",
    "    # Choose three groups; one of them must be a 'capital' group.\n",
    "    selected_groups = {}\n",
    "    capital_group_key = None\n",
    "    \n",
    "    # Look for a group name containing 'capital' (case-insensitive)\n",
    "    for key in groups.keys():\n",
    "        if \"capital\" in key.lower():\n",
    "            capital_group_key = key\n",
    "            break\n",
    "    if capital_group_key is None:\n",
    "        raise ValueError(\"No capital-related group found in the dataset!\")\n",
    "    \n",
    "    selected_groups[capital_group_key] = groups[capital_group_key]\n",
    "    \n",
    "    # Add any two other groups (excluding the already selected capital group)\n",
    "    count = 1\n",
    "    for key in groups.keys():\n",
    "        if key == capital_group_key:\n",
    "            continue\n",
    "        if count >= 3:\n",
    "            break\n",
    "        selected_groups[key] = groups[key]\n",
    "        count += 1\n",
    "    \n",
    "    # Evaluate each selected group and output the results table\n",
    "    for group_name, analogy_list in selected_groups.items():\n",
    "        ks, cosine_acc, l2_acc = evaluate_group(analogy_list)\n",
    "        print_table(group_name, ks, cosine_acc, l2_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bdd61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41122453",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda587d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a22c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ce640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_4613",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
