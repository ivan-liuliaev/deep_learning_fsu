{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a467d87",
   "metadata": {},
   "source": [
    "# Project #2 â€“ Transformers for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7643ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24c0145b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 512)\n",
       "    (token_type_embeddings): Embedding(2, 512)\n",
       "    (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-3): 4 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the lightweight BERT model and its tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
    "model = AutoModel.from_pretrained(\"prajjwal1/bert-small\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411c2d7",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a35d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: capital-common-countries\n",
      "k     Accuracy (Cosine Similarity)   Accuracy (L2 Distance)        \n",
      "1     16.80                          89.13                         \n",
      "2     38.34                          96.84                         \n",
      "5     59.88                          98.81                         \n",
      "10    80.24                          100.00                        \n",
      "20    96.64                          100.00                        \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Group: capital-world\n",
      "k     Accuracy (Cosine Similarity)   Accuracy (L2 Distance)        \n",
      "1     5.04                           34.88                         \n",
      "2     11.45                          52.14                         \n",
      "5     18.35                          64.83                         \n",
      "10    26.02                          73.05                         \n",
      "20    37.44                          81.61                         \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Group: currency\n",
      "k     Accuracy (Cosine Similarity)   Accuracy (L2 Distance)        \n",
      "1     0.00                           0.00                          \n",
      "2     4.27                           7.16                          \n",
      "5     16.51                          27.48                         \n",
      "10    36.26                          51.27                         \n",
      "20    71.94                          80.83                         \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_cache = {}\n",
    "def get_embedding(word):\n",
    "    word = word.lower()\n",
    "    if word in embedding_cache:\n",
    "        return embedding_cache[word]\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(word, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        token_embeds = outputs.last_hidden_state.squeeze(0)  \n",
    "        if token_embeds.dim() == 1:\n",
    "            embed = token_embeds\n",
    "        else:\n",
    "            embed = token_embeds.mean(dim=0)\n",
    "        embedding_cache[word] = embed\n",
    "        return embed\n",
    "\n",
    "def read_analogy_file(file_path):\n",
    "    groups = {}\n",
    "    current_group = None\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\":\"):\n",
    "                group_name = line[1:].strip()\n",
    "                current_group = group_name\n",
    "                groups[current_group] = []\n",
    "            else:\n",
    "                tokens = line.split()\n",
    "                if len(tokens) == 4 and current_group is not None:\n",
    "                    groups[current_group].append(tokens)\n",
    "    return groups\n",
    "\n",
    "def evaluate_group(analogy_list):\n",
    "    for tokens in analogy_list:\n",
    "        candidates.add(tokens[1].lower())\n",
    "        candidates.add(tokens[3].lower())\n",
    "    candidates = list(candidates)\n",
    "\n",
    "    ks = [1, 2, 5, 10, 20]\n",
    "    cosine_correct = {k: 0 for k in ks}\n",
    "    l2_correct = {k: 0 for k in ks}\n",
    "    total = len(analogy_list)\n",
    "    \n",
    "    for tokens in analogy_list:\n",
    "        a, b, c, d = [word.lower() for word in tokens]\n",
    "        ref_diff = get_embedding(a) - get_embedding(b)\n",
    "        \n",
    "\n",
    "        cos_scores = {}\n",
    "        l2_scores = {}\n",
    "        for cand in candidates:\n",
    "            candidate_diff = get_embedding(c) - get_embedding(cand)\n",
    "            cos_sim = F.cosine_similarity(ref_diff, candidate_diff, dim=0)\n",
    "\n",
    "            l2_distance = torch.norm(ref_diff - candidate_diff, p=2)\n",
    "            cos_scores[cand] = cos_sim.item()\n",
    "            l2_scores[cand] = l2_distance.item()\n",
    "        \n",
    "\n",
    "        sorted_cos = sorted(cos_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        sorted_l2 = sorted(l2_scores.items(), key=lambda x: x[1])\n",
    "        \n",
    "\n",
    "        for k in ks:\n",
    "            top_k_cos = [item[0] for item in sorted_cos[:k]]\n",
    "            top_k_l2 = [item[0] for item in sorted_l2[:k]]\n",
    "            if d in top_k_cos:\n",
    "                cosine_correct[k] += 1\n",
    "            if d in top_k_l2:\n",
    "                l2_correct[k] += 1\n",
    "                \n",
    "\n",
    "    cosine_acc = {k: (cosine_correct[k] / total) * 100 for k in ks}\n",
    "    l2_acc = {k: (l2_correct[k] / total) * 100 for k in ks}\n",
    "    \n",
    "    return ks, cosine_acc, l2_acc\n",
    "\n",
    "def print_table(group_name, ks, cosine_acc, l2_acc):\n",
    "    print(\"Group:\", group_name)\n",
    "    print(\"{:<5} {:<30} {:<30}\".format(\"k\", \"Accuracy (Cosine Similarity)\", \"Accuracy (L2 Distance)\"))\n",
    "    for k in ks:\n",
    "        print(\"{:<5} {:<30.2f} {:<30.2f}\".format(k, cosine_acc[k], l2_acc[k]))\n",
    "    print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"./data/task_1_data.txt\"\n",
    "    groups = read_analogy_file(file_path)\n",
    "    selected_groups = {}\n",
    "    capital_group_key = None\n",
    "    \n",
    "    for key in groups.keys():\n",
    "        if \"capital\" in key.lower():\n",
    "            capital_group_key = key\n",
    "            break\n",
    "    if capital_group_key is None:\n",
    "        raise ValueError(\"No capital-related group found in the dataset!\")\n",
    "    \n",
    "    selected_groups[capital_group_key] = groups[capital_group_key]\n",
    "    \n",
    "\n",
    "    count = 1\n",
    "    for key in groups.keys():\n",
    "        if key == capital_group_key:\n",
    "            continue\n",
    "        if count >= 3:\n",
    "            break\n",
    "        selected_groups[key] = groups[key]\n",
    "        count += 1    \n",
    "    for group_name, analogy_list in selected_groups.items():\n",
    "        ks, cosine_acc, l2_acc = evaluate_group(analogy_list)\n",
    "        print_table(group_name, ks, cosine_acc, l2_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0993a25",
   "metadata": {},
   "source": [
    "#### **note: the numbers may slightly differ from the ones put in the report, due to switching between bert-tiny, bert-mini and bert-small during testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41122453",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660a127",
   "metadata": {},
   "source": [
    "### Initial implementation: Logistic Regression and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for training samples...\n",
      "Computing embeddings for test samples...\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 77.62%\n",
      "Classification Report (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.48      0.47        50\n",
      "         1.0       0.08      0.05      0.06        20\n",
      "         2.0       0.17      0.16      0.17        25\n",
      "         3.0       0.20      0.10      0.14       106\n",
      "         4.0       0.86      0.92      0.89       782\n",
      "\n",
      "    accuracy                           0.78       983\n",
      "   macro avg       0.35      0.34      0.34       983\n",
      "weighted avg       0.74      0.78      0.75       983\n",
      "\n",
      "\n",
      "Training XGBoost Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cap_4613/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:48:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 79.96%\n",
      "Classification Report (XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.24      0.34        50\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "         2.0       0.00      0.00      0.00        25\n",
      "         3.0       0.00      0.00      0.00       106\n",
      "         4.0       0.81      0.99      0.89       782\n",
      "\n",
      "    accuracy                           0.80       983\n",
      "   macro avg       0.28      0.25      0.25       983\n",
      "weighted avg       0.67      0.80      0.73       983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_sentence_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # first token of the sequence\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return cls_embedding.squeeze(0).cpu().numpy()\n",
    "\n",
    "def compute_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        emb = get_sentence_embedding(text)\n",
    "        embeddings.append(emb)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"./data/amazon_reviews.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "texts = data[\"reviewText\"].astype(str).tolist()\n",
    "labels = data[\"overall\"].values  # original labels in range 1-5\n",
    "\n",
    "# Convert ratings to zero-indexed labels for XGBoost to handle them properly\n",
    "labels = labels - 1\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_embeddings = compute_embeddings(X_train)\n",
    "test_embeddings = compute_embeddings(X_test)\n",
    "\n",
    "\n",
    "# Model 1: Logistic Regression\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(train_embeddings, y_train)\n",
    "y_pred_lr = lr_model.predict(test_embeddings)\n",
    "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Logistic Regression Accuracy: {:.2f}%\".format(lr_acc * 100))\n",
    "print(\"Classification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "\n",
    "# Model 2: XGBoost Classifier\n",
    "print(\"\\nTraining XGBoost Classifier...\")\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(train_embeddings, y_train)\n",
    "y_pred_xgb = xgb_model.predict(test_embeddings)\n",
    "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"XGBoost Accuracy: {:.2f}%\".format(xgb_acc * 100))\n",
    "print(\"Classification Report (XGBoost):\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c547a",
   "metadata": {},
   "source": [
    "### Experiment: Tweaked Logistic Regression to address class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a22c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 61.55%\n",
      "Classification Report (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.44      0.38        50\n",
      "         1.0       0.15      0.25      0.19        20\n",
      "         2.0       0.13      0.32      0.19        25\n",
      "         3.0       0.18      0.41      0.25       106\n",
      "         4.0       0.90      0.67      0.77       782\n",
      "\n",
      "    accuracy                           0.62       983\n",
      "   macro avg       0.34      0.42      0.35       983\n",
      "weighted avg       0.76      0.62      0.67       983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "lr_model.fit(train_embeddings, y_train)\n",
    "y_pred_lr = lr_model.predict(test_embeddings)\n",
    "lr_acc = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Logistic Regression Accuracy: {:.2f}%\".format(lr_acc * 100))\n",
    "print(\"Classification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cd719",
   "metadata": {},
   "source": [
    "### Experiment: Tweaked XGBoost to make it generalize better (reduce overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d563e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost Classifier...\n",
      "[0]\tvalidation_0-mlogloss:1.46149\tvalidation_1-mlogloss:1.46535\n",
      "[1]\tvalidation_0-mlogloss:1.34245\tvalidation_1-mlogloss:1.35067\n",
      "[2]\tvalidation_0-mlogloss:1.24347\tvalidation_1-mlogloss:1.25597\n",
      "[3]\tvalidation_0-mlogloss:1.16051\tvalidation_1-mlogloss:1.17752\n",
      "[4]\tvalidation_0-mlogloss:1.09013\tvalidation_1-mlogloss:1.11197\n",
      "[5]\tvalidation_0-mlogloss:1.02931\tvalidation_1-mlogloss:1.05592\n",
      "[6]\tvalidation_0-mlogloss:0.97626\tvalidation_1-mlogloss:1.00674\n",
      "[7]\tvalidation_0-mlogloss:0.92974\tvalidation_1-mlogloss:0.96387\n",
      "[8]\tvalidation_0-mlogloss:0.88853\tvalidation_1-mlogloss:0.92721\n",
      "[9]\tvalidation_0-mlogloss:0.85155\tvalidation_1-mlogloss:0.89446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cap_4613/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:49:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalidation_0-mlogloss:0.81848\tvalidation_1-mlogloss:0.86589\n",
      "[11]\tvalidation_0-mlogloss:0.78927\tvalidation_1-mlogloss:0.84063\n",
      "[12]\tvalidation_0-mlogloss:0.76276\tvalidation_1-mlogloss:0.81903\n",
      "[13]\tvalidation_0-mlogloss:0.73885\tvalidation_1-mlogloss:0.79888\n",
      "[14]\tvalidation_0-mlogloss:0.71742\tvalidation_1-mlogloss:0.78200\n",
      "[15]\tvalidation_0-mlogloss:0.69762\tvalidation_1-mlogloss:0.76591\n",
      "[16]\tvalidation_0-mlogloss:0.68049\tvalidation_1-mlogloss:0.75197\n",
      "[17]\tvalidation_0-mlogloss:0.66400\tvalidation_1-mlogloss:0.73928\n",
      "[18]\tvalidation_0-mlogloss:0.64930\tvalidation_1-mlogloss:0.72823\n",
      "[19]\tvalidation_0-mlogloss:0.63493\tvalidation_1-mlogloss:0.71813\n",
      "[20]\tvalidation_0-mlogloss:0.62203\tvalidation_1-mlogloss:0.70908\n",
      "[21]\tvalidation_0-mlogloss:0.61020\tvalidation_1-mlogloss:0.70030\n",
      "[22]\tvalidation_0-mlogloss:0.59833\tvalidation_1-mlogloss:0.69233\n",
      "[23]\tvalidation_0-mlogloss:0.58791\tvalidation_1-mlogloss:0.68599\n",
      "[24]\tvalidation_0-mlogloss:0.57820\tvalidation_1-mlogloss:0.68002\n",
      "[25]\tvalidation_0-mlogloss:0.56882\tvalidation_1-mlogloss:0.67463\n",
      "[26]\tvalidation_0-mlogloss:0.56003\tvalidation_1-mlogloss:0.66997\n",
      "[27]\tvalidation_0-mlogloss:0.55180\tvalidation_1-mlogloss:0.66529\n",
      "[28]\tvalidation_0-mlogloss:0.54418\tvalidation_1-mlogloss:0.66048\n",
      "[29]\tvalidation_0-mlogloss:0.53640\tvalidation_1-mlogloss:0.65658\n",
      "[30]\tvalidation_0-mlogloss:0.52896\tvalidation_1-mlogloss:0.65295\n",
      "[31]\tvalidation_0-mlogloss:0.52176\tvalidation_1-mlogloss:0.64942\n",
      "[32]\tvalidation_0-mlogloss:0.51456\tvalidation_1-mlogloss:0.64588\n",
      "[33]\tvalidation_0-mlogloss:0.50789\tvalidation_1-mlogloss:0.64344\n",
      "[34]\tvalidation_0-mlogloss:0.50183\tvalidation_1-mlogloss:0.64068\n",
      "[35]\tvalidation_0-mlogloss:0.49564\tvalidation_1-mlogloss:0.63858\n",
      "[36]\tvalidation_0-mlogloss:0.48966\tvalidation_1-mlogloss:0.63596\n",
      "[37]\tvalidation_0-mlogloss:0.48388\tvalidation_1-mlogloss:0.63406\n",
      "[38]\tvalidation_0-mlogloss:0.47846\tvalidation_1-mlogloss:0.63191\n",
      "[39]\tvalidation_0-mlogloss:0.47330\tvalidation_1-mlogloss:0.63035\n",
      "[40]\tvalidation_0-mlogloss:0.46865\tvalidation_1-mlogloss:0.62830\n",
      "[41]\tvalidation_0-mlogloss:0.46357\tvalidation_1-mlogloss:0.62682\n",
      "[42]\tvalidation_0-mlogloss:0.45905\tvalidation_1-mlogloss:0.62558\n",
      "[43]\tvalidation_0-mlogloss:0.45382\tvalidation_1-mlogloss:0.62371\n",
      "[44]\tvalidation_0-mlogloss:0.44937\tvalidation_1-mlogloss:0.62232\n",
      "[45]\tvalidation_0-mlogloss:0.44480\tvalidation_1-mlogloss:0.62111\n",
      "[46]\tvalidation_0-mlogloss:0.44008\tvalidation_1-mlogloss:0.62007\n",
      "[47]\tvalidation_0-mlogloss:0.43554\tvalidation_1-mlogloss:0.61894\n",
      "[48]\tvalidation_0-mlogloss:0.43096\tvalidation_1-mlogloss:0.61740\n",
      "[49]\tvalidation_0-mlogloss:0.42682\tvalidation_1-mlogloss:0.61680\n",
      "\n",
      "XGBoost - Training Accuracy: 84.28%\n",
      "Classification Report (XGBoost on Training Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.49      0.63       194\n",
      "         1.0       1.00      0.47      0.64        60\n",
      "         2.0       1.00      0.22      0.36       117\n",
      "         3.0       0.85      0.07      0.12       421\n",
      "         4.0       0.84      1.00      0.91      3140\n",
      "\n",
      "    accuracy                           0.84      3932\n",
      "   macro avg       0.92      0.45      0.53      3932\n",
      "weighted avg       0.85      0.84      0.79      3932\n",
      "\n",
      "\n",
      "XGBoost - Test Accuracy: 79.86%\n",
      "Classification Report (XGBoost on Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      0.14      0.22        50\n",
      "         1.0       0.00      0.00      0.00        20\n",
      "         2.0       0.00      0.00      0.00        25\n",
      "         3.0       0.00      0.00      0.00       106\n",
      "         4.0       0.81      0.99      0.89       782\n",
      "\n",
      "    accuracy                           0.80       983\n",
      "   macro avg       0.25      0.23      0.22       983\n",
      "weighted avg       0.67      0.80      0.72       983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cap_4613/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/cap_4613/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/cap_4613/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# tuned hyperparameters for better generalization\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    max_depth=3,           \n",
    "    learning_rate=0.1,     \n",
    "    n_estimators=50,      \n",
    "    subsample=0.8,         \n",
    "    colsample_bytree=0.8,  \n",
    "    reg_alpha=0.1,         \n",
    "    reg_lambda=1.0         \n",
    ")\n",
    "xgb_model.fit(\n",
    "    train_embeddings, \n",
    "    y_train, \n",
    "    eval_set=[(train_embeddings, y_train), (test_embeddings, y_test)],\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on training set\n",
    "y_pred_train_xgb = xgb_model.predict(train_embeddings)\n",
    "train_acc_xgb = accuracy_score(y_train, y_pred_train_xgb)\n",
    "print(\"\\nXGBoost - Training Accuracy: {:.2f}%\".format(train_acc_xgb * 100))\n",
    "print(\"Classification Report (XGBoost on Training Set):\")\n",
    "print(classification_report(y_train, y_pred_train_xgb))\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_xgb = xgb_model.predict(test_embeddings)\n",
    "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"\\nXGBoost - Test Accuracy: {:.2f}%\".format(xgb_acc * 100))\n",
    "print(\"Classification Report (XGBoost on Test Set):\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_4613",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
