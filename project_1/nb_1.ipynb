{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# CONSTANTS FOR EXPERIMENT TWEAKING\n",
    "# ----------------------------\n",
    "TRAIN_FILE = 'zip_train.txt'       # path to training file\n",
    "TEST_FILE = 'zip_test.txt'         # path to test file\n",
    "BATCH_SIZE = 64                    # Batch size (affects batch normalization performance)\n",
    "EPOCHS = 7                        # Number of training epochs\n",
    "LEARNING_RATE = 0.03             # Learning rate for SGD optimizer\n",
    "MOMENTUM = 0.90                     # Momentum for SGD optimizer (e.g., 0.5, 0.9, or 0.99)\n",
    "DROPOUT_RATE = 0.5                 # Dropout rate for fully connected layer (0.0 to 1.0)\n",
    "INIT_STRATEGY = 'effective'        # Weight initialization strategy: 'slow', 'effective', or 'too_fast'\n",
    "USE_ENSEMBLE = True               # Set to True to train an ensemble of 3 CNN models\n",
    "\n",
    "# ----------------------------\n",
    "# Data Processing\n",
    "# ----------------------------\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the digit dataset from a text file.\n",
    "    Each row: <label> <256 feature values>\n",
    "    Reshape each sample into a 16x16 image.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            if len(tokens) != 257:  # 1 label + 256 features\n",
    "                continue\n",
    "            label = int(float(tokens[0]))  # class label (0-9)\n",
    "            features = np.array(tokens[1:], dtype=np.float32)\n",
    "            image = features.reshape(16, 16)  # 16x16 image\n",
    "            data.append((image, label))\n",
    "    return data\n",
    "\n",
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Add channel dimension: shape becomes (1, 16, 16)\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "# ----------------------------\n",
    "# CNN Model Definition\n",
    "# ----------------------------\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, dropout_rate=DROPOUT_RATE):\n",
    "        \"\"\"\n",
    "        The CNN has:\n",
    "         - Three convolutional layers (with weight sharing over spatial locations)\n",
    "         - Batch normalization after each conv layer\n",
    "         - ReLU activations after the first two conv layers\n",
    "         - Tanh activation after the third conv layer (satisfying the sigmoid/tanh requirement)\n",
    "         - Adaptive pooling to reduce feature map size\n",
    "         - One fully connected hidden layer with dropout before the final classification layer\n",
    "        \"\"\"\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.tanh = nn.Tanh()  # using tanh here\n",
    "        \n",
    "        # Adaptive pooling to get a fixed feature map size (e.g., 4x4)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(256, 10)  # 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Parameter Initialization Strategies\n",
    "# ----------------------------\n",
    "def initialize_weights(model, strategy=INIT_STRATEGY):\n",
    "    \"\"\"\n",
    "    Initialize weights based on the chosen strategy:\n",
    "      - 'slow': small weights (std=0.001) → learning very slow.\n",
    "      - 'effective': Kaiming (He) normal initialization.\n",
    "      - 'too_fast': high variance weights (std=1.0) → learning too fast.\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            if strategy == 'slow':\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.001)\n",
    "            elif strategy == 'effective':\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif strategy == 'too_fast':\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=1.0)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# ----------------------------\n",
    "# Training and Evaluation Functions\n",
    "# ----------------------------\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Epoch {epoch}: Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# ----------------------------\n",
    "# Ensemble Prediction Function\n",
    "# ----------------------------\n",
    "def ensemble_predict(models, device, data_loader):\n",
    "    \"\"\"\n",
    "    Given a list of trained models, average their outputs for ensemble prediction.\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            ensemble_outputs = 0\n",
    "            for model in models:\n",
    "                outputs = model(inputs)\n",
    "                ensemble_outputs += outputs\n",
    "            avg_outputs = ensemble_outputs / len(models)\n",
    "            _, predicted = avg_outputs.max(1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    return np.array(all_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training single CNN model...\n",
      "Epoch 1: Train Loss: 0.2770, Train Accuracy: 91.51%\n",
      "Test Loss: 0.2339, Test Accuracy: 94.37%\n",
      "Epoch 2: Train Loss: 0.0708, Train Accuracy: 97.74%\n",
      "Test Loss: 0.1960, Test Accuracy: 95.02%\n",
      "Epoch 3: Train Loss: 0.0481, Train Accuracy: 98.44%\n",
      "Test Loss: 0.1799, Test Accuracy: 95.81%\n",
      "Epoch 4: Train Loss: 0.0387, Train Accuracy: 98.75%\n",
      "Test Loss: 0.1755, Test Accuracy: 96.06%\n",
      "Epoch 5: Train Loss: 0.0293, Train Accuracy: 99.16%\n",
      "Test Loss: 0.1868, Test Accuracy: 95.91%\n",
      "Epoch 6: Train Loss: 0.0260, Train Accuracy: 99.30%\n",
      "Test Loss: 0.1699, Test Accuracy: 96.66%\n",
      "Epoch 7: Train Loss: 0.0167, Train Accuracy: 99.55%\n",
      "Test Loss: 0.1873, Test Accuracy: 96.26%\n",
      "\n",
      "Training ensemble of 3 CNN models...\n",
      "\n",
      "Training model 1 of the ensemble:\n",
      "Epoch 1: Train Loss: 0.2563, Train Accuracy: 91.77%\n",
      "Epoch 2: Train Loss: 0.0877, Train Accuracy: 97.56%\n",
      "Epoch 3: Train Loss: 0.0541, Train Accuracy: 98.29%\n",
      "Epoch 4: Train Loss: 0.0423, Train Accuracy: 98.61%\n",
      "Epoch 5: Train Loss: 0.0301, Train Accuracy: 98.99%\n",
      "Epoch 6: Train Loss: 0.0282, Train Accuracy: 99.16%\n",
      "Epoch 7: Train Loss: 0.0209, Train Accuracy: 99.40%\n",
      "\n",
      "Training model 2 of the ensemble:\n",
      "Epoch 1: Train Loss: 0.2599, Train Accuracy: 91.70%\n",
      "Epoch 2: Train Loss: 0.0721, Train Accuracy: 97.74%\n",
      "Epoch 3: Train Loss: 0.0495, Train Accuracy: 98.57%\n",
      "Epoch 4: Train Loss: 0.0414, Train Accuracy: 98.68%\n",
      "Epoch 5: Train Loss: 0.0327, Train Accuracy: 98.88%\n",
      "Epoch 6: Train Loss: 0.0240, Train Accuracy: 99.30%\n",
      "Epoch 7: Train Loss: 0.0198, Train Accuracy: 99.51%\n",
      "\n",
      "Training model 3 of the ensemble:\n",
      "Epoch 1: Train Loss: 0.2423, Train Accuracy: 92.10%\n",
      "Epoch 2: Train Loss: 0.0779, Train Accuracy: 97.61%\n",
      "Epoch 3: Train Loss: 0.0466, Train Accuracy: 98.52%\n",
      "Epoch 4: Train Loss: 0.0391, Train Accuracy: 98.96%\n",
      "Epoch 5: Train Loss: 0.0290, Train Accuracy: 99.04%\n",
      "Epoch 6: Train Loss: 0.0217, Train Accuracy: 99.31%\n",
      "Epoch 7: Train Loss: 0.0212, Train Accuracy: 99.36%\n",
      "\n",
      "Ensemble Test Accuracy: 96.36%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load training and test data\n",
    "train_data = load_data(TRAIN_FILE)\n",
    "test_data = load_data(TEST_FILE)\n",
    "train_dataset = DigitDataset(train_data)\n",
    "test_dataset = DigitDataset(test_data)\n",
    "\n",
    "# Create DataLoader with specified batch size (this impacts batch normalization statistics)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Create and initialize the model\n",
    "# ----------------------------\n",
    "model = ConvNet(dropout_rate=DROPOUT_RATE).to(device)\n",
    "initialize_weights(model, strategy=INIT_STRATEGY)\n",
    "\n",
    "# Loss function and optimizer (using SGD with specified learning rate and momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "print(\"Training single CNN model...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_loader, criterion)\n",
    "\n",
    "# ----------------------------\n",
    "# Ensemble Experiment (if enabled)\n",
    "# ----------------------------\n",
    "if USE_ENSEMBLE:\n",
    "    print(\"\\nTraining ensemble of 3 CNN models...\")\n",
    "    models = []\n",
    "    for i in range(3):\n",
    "        print(f\"\\nTraining model {i+1} of the ensemble:\")\n",
    "        m = ConvNet(dropout_rate=DROPOUT_RATE).to(device)\n",
    "        initialize_weights(m, strategy=INIT_STRATEGY)\n",
    "        opt = optim.SGD(m.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "        for epoch in range(1, EPOCHS + 1):\n",
    "            train(m, device, train_loader, opt, criterion, epoch)\n",
    "        models.append(m)\n",
    "    # Ensemble prediction on test set\n",
    "    ensemble_preds = ensemble_predict(models, device, test_loader)\n",
    "    true_labels = []\n",
    "    for _, targets in test_loader:\n",
    "        true_labels.extend(targets.numpy())\n",
    "    true_labels = np.array(true_labels)\n",
    "    ensemble_accuracy = (ensemble_preds == true_labels).mean() * 100\n",
    "    print(f\"\\nEnsemble Test Accuracy: {ensemble_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_4613",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
